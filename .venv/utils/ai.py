# utils/ai.py

import os
import json
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()
API_KEY = os.environ.get("API_KEY") or os.environ.get("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("Please set API_KEY or GEMINI_API_KEY in your environment.")

# Asia/Kolkata timezone
TZ = timezone(timedelta(hours=5, minutes=30))

class AIIntentParser:
    def __init__(self, model_name="learnlm-2.0-flash-experimental"):
        self.client = genai.Client(api_key=API_KEY)
        self.model = model_name
        self.prompt_history = []
        self.max_history = 10

        # Build the system prompt once
        now_iso = datetime.now(TZ).isoformat(timespec="seconds")
        self.system_prompt = f'''
You are an AI assistant for a Google Workspace CLI.
Current date/time (Asia/Kolkata): {now_iso}

Parse the user‚Äôs request into **one** JSON object with:
  ‚Ä¢ "service": "gmail" | "calendar" | "chat"
  ‚Ä¢ "actions": [ {{ "action": "<name>", "parameters": {{ ... }} }}, ... ]

**Supported Gmail actions**:
  ‚Ä¢ send             ‚Üí to (string or [strings]), subject (string), body (string), html (opt), attachments (opt)
  ‚Ä¢ list             ‚Üí count (int, opt), query/q (string, opt)
  ‚Ä¢ summarize        ‚Üí count (int, opt)
  ‚Ä¢ read             ‚Üí id (string, opt) OR count (int, opt)
  ‚Ä¢ attachments_info ‚Üí id (string, required)
  ‚Ä¢ search           ‚Üí query (string, required), max_results (int, opt)
  ‚Ä¢ list_labels      ‚Üí (no parameters)
  ‚Ä¢ create_label     ‚Üí name (string, required)
  ‚Ä¢ update_label     ‚Üí id (string, required), name (string, required)
  ‚Ä¢ delete_label     ‚Üí id (string, required)
  ‚Ä¢ list_by_label    ‚Üí label_ids ([strings], required), count (int, opt)
  ‚Ä¢ mark_read        ‚Üí id (string, required)
  ‚Ä¢ mark_unread      ‚Üí id (string, required)
  ‚Ä¢ move             ‚Üí id (string, required), label_id (string, required)
  ‚Ä¢ delete           ‚Üí id (string, opt) OR ids ([strings], opt)
  ‚Ä¢ batch_mark_read  ‚Üí ids ([strings], required)
  ‚Ä¢ batch_delete     ‚Üí ids ([strings], required)

**Supported Calendar actions**:
  ‚Ä¢ list     ‚Üí count (int, opt)
  ‚Ä¢ create   ‚Üí either:
       ‚Äì start (RFC3339 string) & end (RFC3339 string)
       ‚Äì date (\"YYYY-MM-DD\" or \"tomorrow\") & summary (string, opt) & description (opt) & time (\"3pm\" style, opt)
    If only date is given, default to a 1‚Äëhour slot 09:00‚Äì10:00 local time.

**Multi‚Äëaction sequencing**:
  If the user requests multiple tasks (e.g. ‚ÄúSend an email, then list my last 3‚Äù), list them in order in "actions".

**Chat fallback**:
  If the request is not about Gmail or Calendar, return:
    {{ "service":"chat", "actions":[] }}

Output **only** the JSON‚Äîno extra text.
'''

    def parse_prompt(self, user_prompt: str) -> dict | None:
        # Manage history
        self.prompt_history.append(user_prompt)
        if len(self.prompt_history) > self.max_history:
            self.prompt_history.clear()
            print(f"üóëÔ∏è Prompt history cleared after {self.max_history} entries.")

        # Prepare and stream
        full_prompt = self.system_prompt + "\nUser: " + user_prompt
        response_text = ""
        for chunk in self.client.models.generate_content_stream(
            model=self.model,
            contents=[types.Content(role="user",
                                    parts=[types.Part.from_text(text=full_prompt)])],
            config=types.GenerateContentConfig(response_mime_type="application/json")
        ):
            if chunk.text:
                print(chunk.text, end="")
                response_text += chunk.text

        # Attempt JSON parse
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            # Chat fallback
            print("\nü§ñ (chat fallback)")
            for chunk in self.client.models.generate_content_stream(
                model=self.model,
                contents=[types.Content(role="user",
                                        parts=[types.Part.from_text(text=user_prompt)])]
            ):
                print(chunk.text, end="")
            return None
        
    def chat_ai(self,prompt: str) -> str:
      try:
          
          client = genai.Client(api_key=API_KEY)

          contents = [
              types.Content(
                  role="user",
                  parts=[types.Part.from_text(text=prompt)],
              )
          ]
         
          # Optional: System instruction
          system_instruction = [
              types.Part.from_text(
                  text="You are a helpful assistant for general queries. Respond clearly and concisely."
              )
          ]
          
          config = types.GenerateContentConfig(
              response_mime_type="text/plain",
              system_instruction=system_instruction,
          )

          response_text = ""
          for chunk in client.models.generate_content_stream(
              model="learnlm-2.0-flash-experimental",
              contents=contents,
              config=config,
          ):
              if chunk.text:
                  print(chunk.text, end="")  # Optional: live stream to console
                  response_text += chunk.text

          return response_text

      except Exception as e:
          return f"‚ùå Chat error: {e}"
